{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3083192e-64de-4fac-88b9-ba38aed52bd6",
   "metadata": {},
   "source": [
    "### **Description**\n",
    "\n",
    "Fantastic work on the Data Analysis & EDA! You've cleaned the data and know it well. Now, it's time to use that data to **build, train, and test our predictive models**.\n",
    "\n",
    "This is where we go from being a data analyst to a data scientist. Your goal is to experiment with different techniques to find the best-performing machine learning model that can accurately predict whether a patient has heart disease.\n",
    "\n",
    "---\n",
    "\n",
    "### **Checklist & Key Steps**\n",
    "\n",
    "**1. Final Data Preprocessing (The Experiment)**\n",
    "This is a critical step where your choices will directly impact the model.\n",
    "* **Handle Categorical Data (The Challenge):**\n",
    "    * Your models need numbers, not text. You must convert categorical columns (like 'sex', 'cp', 'fbs', etc.) into numeric ones.\n",
    "    * **I do not want you to use just one method.** I want you to experiment and see the results. You must try at least **two** different encoding strategies and compare their impact. For example:\n",
    "        * **Strategy A: One-Hot Encoding** (e.g., using `pd.get_dummies`). This creates new \"dummy\" columns for each category.\n",
    "        * **Strategy B: Label Encoding** (e.g., using `LabelEncoder`). This converts each category into a single number (e.g., 'Male'=0, 'Female'=1).\n",
    "* **Apply Your Cleaning Strategy:** Implement the plan you made for handling outliers or transformed features from Task 1.\n",
    "* **Feature Scaling:** This is critical! Many models perform poorly if features are on different scales. Use `StandardScaler` from scikit-learn to scale your numeric features. (Note: Apply scaling *after* your train/test split to avoid data leakage!)\n",
    "\n",
    "**2. Prepare for Training**\n",
    "* **Define X and y:** Separate your data into `X` (all the features/columns for predicting) and `y` (the 'target' column you are trying to predict).\n",
    "* **Train/Test Split:** Split your `X` and `y` data into a training set and a testing set (e.g., 80% for training, 20% for testing) using `train_test_split`. **Do this *before* scaling.**\n",
    "\n",
    "**3. Model Training (Baseline)**\n",
    "Let's train a few different types of models to see what works best. Train these models using the default settings for *each* of your encoding strategies (e.g., train a Logistic Regression on your One-Hot-Encoded data, and *another* Logistic Regression on your Label-Encoded data).\n",
    "* Train a **Logistic Regression** model.\n",
    "* Train a **Desicion tree** model.\n",
    "* Train a **Support Vector Machine (SVM)** model.\n",
    "* Train a **Random Forest Classifier** model.\n",
    "\n",
    "**4. Model Evaluation (Baseline)**\n",
    "For each model you trained, use the *test set* to see how well it performs. We need to know more than just \"accuracy.\"\n",
    "* **Accuracy:** How many predictions were correct overall?\n",
    "* **Precision, Recall, and F1-Score:** Get these from a `classification_report`.\n",
    "    * **Recall** is very important here: How many *actual* heart disease cases did we catch? (We don't want to miss any!)\n",
    "* **Confusion Matrix:** Create one for each model. This shows you exactly *what types* of mistakes the model is making.\n",
    "\n",
    "* **ROC-AUC Score:** This gives you a single number to judge how well a model can separate the two classes (disease vs. no disease).\n",
    "\n",
    "**5. Hyperparameter Tuning**\n",
    "* **Select Your Best Combination:** Based on your evaluation (especially Recall and ROC-AUC scores), pick your **best-performing combination** of (Encoding Strategy + Model).\n",
    "* **Tune:** Use `GridSearchCV` or `RandomizedSearchCV` to automatically find the *best settings* (hyperparameters) for your chosen model to maximize its performance.\n",
    "\n",
    "**6. Final Evaluation & Feature Importance**\n",
    "* **Final Scores:** Evaluate your *tuned* model on the test set. How much did its scores improve from the baseline?\n",
    "* **Feature Importance:** Now, let's look *inside* the model.\n",
    "    * If you used Logistic Regression or SVM, look at the `model.coef_` attribute.\n",
    "    * If you used Random Forest, look at the `model.feature_importances_` attribute.\n",
    "* Create a bar chart to visualize the top 5-10 most important features. What does the model \"think\" is the best predictor of heart disease?\n",
    "\n",
    "---\n",
    "\n",
    "### **Deliverables**\n",
    "\n",
    "* A github link includes your jubyter notebook containing all your code for preprocessing, training, and evaluation.\n",
    "* **Important:** Please include text/markdown cells with your observations:\n",
    "    * **Crucially: How did your choice of *categorical encoding* (Strategy A vs. B) affect your models' performance? Which was better and why do you think that is?**\n",
    "    * Which model performed best at the baseline?\n",
    "    * How much did hyperparameter tuning help?\n",
    "    * Which model is your *final choice* and **why**?\n",
    "    * What were the most important features your model found? Did they match your hypotheses from Task 1?\n",
    "\n",
    "---\n",
    "\n",
    "### **Resources**\n",
    "\n",
    "* **Tools:** You will be using **scikit-learn** for almost everything here.\n",
    "* **Key functions to look up:**\n",
    "    * `pd.get_dummies` (for One-Hot)\n",
    "    * `LabelEncoder`\n",
    "    * `train_test_split`\n",
    "    * `StandardScaler`\n",
    "    * `LogisticRegression`, `KNeighborsClassifier`, `SVC`, `RandomForestClassifier`\n",
    "    * `classification_report`, `confusion_matrix`, `roc_auc_score`\n",
    "    * `GridSearchCV`\n",
    " **Start Date:** Friday nov 7 \n",
    "** first follow up: sunday morning nov 9\n",
    "** second follow up: tuesday morning nov 11\n",
    "deadline of the task on wednesday nov 12 at 11:59\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd9739-52f8-4bd6-a238-5e125f8ed9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a811f-87dc-42b7-a4b5-0c58cbcb6853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b6b86-0b18-4ffe-bde7-5a285ba9ff4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd0983-e033-43fc-a7c3-3f3dcdea10f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
